{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad82284-50a0-4c40-b77b-888d32c6d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1+cu121\n",
      "CUDA: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# === CELL 1: Imports ===\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1b4304-033c-4f9a-a413-cab7b9aa1718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences: 19,019\n",
      "Goals: 951 (5.0%)\n",
      "Vocab size: 7\n",
      "\n",
      "First sequence:\n",
      "[{'end_x': 99.0, 'end_y': 74.4, 'type': 'START'}\n",
      " {'end_x': 92.9, 'end_y': 73.9, 'type': 'Carry'}\n",
      " {'end_x': 90.4, 'end_y': 55.2, 'type': 'Pass'}\n",
      " {'end_x': 90.4, 'end_y': 55.4, 'type': 'Carry'}\n",
      " {'end_x': 102.8, 'end_y': 54.8, 'type': 'Pass'}]\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2: Load data ===\n",
    "df = pd.read_parquet('data/sequences_continuous_balanced.parquet')\n",
    "print(f\"Sequences: {len(df):,}\")\n",
    "print(f\"Goals: {df['goal'].sum()} ({df['goal'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Load vocabulary\n",
    "with open('data/vocab_continuous.json', 'r') as f:\n",
    "    type_vocab = json.load(f)\n",
    "\n",
    "with open('data/id_to_type_continuous.json', 'r') as f:\n",
    "    id_to_type = json.load(f)\n",
    "    id_to_type = {int(k): v for k, v in id_to_type.items()}\n",
    "\n",
    "print(f\"Vocab size: {len(type_vocab)}\")\n",
    "print(f\"\\nFirst sequence:\")\n",
    "print(df['events'].iloc[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b49494d1-3ad8-431b-ac93-c6872e17cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Normalizacja w Dataset ===\n",
    "class ContinuousEventDataset(Dataset):\n",
    "    def __init__(self, df, type_vocab, max_length=14):\n",
    "        self.sequences = df['events'].tolist()\n",
    "        self.type_vocab = type_vocab\n",
    "        self.max_length = max_length\n",
    "        self.pad_id = type_vocab['<pad>']\n",
    "        # Normalizacja constants\n",
    "        self.x_max = 120.0\n",
    "        self.y_max = 80.0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        events = self.sequences[idx]\n",
    "        \n",
    "        # Parse + normalize positions\n",
    "        types = [self.type_vocab[e['type']] for e in events]\n",
    "        positions = [\n",
    "            [e['end_x'] / self.x_max if e['end_x'] is not None else 0.0,\n",
    "             e['end_y'] / self.y_max if e['end_y'] is not None else 0.0] \n",
    "            for e in events\n",
    "        ]\n",
    "        has_position = [e['end_x'] is not None for e in events]\n",
    "        \n",
    "        # Causal shift\n",
    "        input_types = types[:-1]\n",
    "        input_positions = positions[:-1]\n",
    "        input_mask = has_position[:-1]\n",
    "        \n",
    "        target_types = types[1:]\n",
    "        target_positions = positions[1:]\n",
    "        target_mask = has_position[1:]\n",
    "        \n",
    "        # Pad\n",
    "        seq_len = len(input_types)\n",
    "        pad_len = self.max_length - 1 - seq_len\n",
    "        \n",
    "        input_types += [self.pad_id] * pad_len\n",
    "        input_positions += [[0.0, 0.0]] * pad_len\n",
    "        input_mask += [False] * pad_len\n",
    "        \n",
    "        target_types += [-100] * pad_len\n",
    "        target_positions += [[0.0, 0.0]] * pad_len\n",
    "        target_mask += [False] * pad_len\n",
    "        \n",
    "        return {\n",
    "            'input_types': torch.tensor(input_types[:self.max_length-1], dtype=torch.long),\n",
    "            'input_positions': torch.tensor(input_positions[:self.max_length-1], dtype=torch.float),\n",
    "            'input_mask': torch.tensor(input_mask[:self.max_length-1], dtype=torch.bool),\n",
    "            'target_types': torch.tensor(target_types[:self.max_length-1], dtype=torch.long),\n",
    "            'target_positions': torch.tensor(target_positions[:self.max_length-1], dtype=torch.float),\n",
    "            'target_mask': torch.tensor(target_mask[:self.max_length-1], dtype=torch.bool)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedf5249-7a68-4dc3-b341-6b424e512caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:    19,019\n",
      "Train:    15,215 (80.0%)\n",
      "Val:      3,804 (20.0%)\n",
      "\n",
      "Train goals: 761 (5.0%)\n",
      "Val goals:   190 (5.0%)\n"
     ]
    }
   ],
   "source": [
    "# === CELL 4: Train/Val Split ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['goal'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total:    {len(df):,}\")\n",
    "print(f\"Train:    {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Val:      {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTrain goals: {train_df['goal'].sum()} ({train_df['goal'].mean()*100:.1f}%)\")\n",
    "print(f\"Val goals:   {val_df['goal'].sum()} ({val_df['goal'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c268ff-7538-4101-9f49-865f4777b02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 951\n",
      "Val batches:   238\n",
      "\n",
      "Batch shapes:\n",
      "  input_types: torch.Size([16, 13])\n",
      "  input_positions: torch.Size([16, 13, 2])\n",
      "  input_mask: torch.Size([16, 13])\n",
      "  target_types: torch.Size([16, 13])\n",
      "  target_positions: torch.Size([16, 13, 2])\n",
      "  target_mask: torch.Size([16, 13])\n"
     ]
    }
   ],
   "source": [
    "# === CELL 5: Create DataLoaders ===\n",
    "train_dataset = ContinuousEventDataset(train_df, type_vocab)\n",
    "val_dataset = ContinuousEventDataset(val_df, type_vocab)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")\n",
    "\n",
    "# Test batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "for k, v in batch.items():\n",
    "    print(f\"  {k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ded007-6ca2-4c4c-b126-83895f50ea5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 38,100,491\n"
     ]
    }
   ],
   "source": [
    "# === CELL 6: Model ===\n",
    "import math\n",
    "\n",
    "class ContinuousEventModel(nn.Module):\n",
    "    def __init__(self, n_types, d_model=256, n_heads=8, n_layers=6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Type embedding\n",
    "        self.type_embedding = nn.Embedding(n_types, d_model)\n",
    "        \n",
    "        # Position encoding (learned MLP)\n",
    "        self.position_encoder = nn.Sequential(\n",
    "            nn.Linear(2, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // 2, d_model)\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Output heads\n",
    "        self.type_head = nn.Linear(d_model, n_types)\n",
    "        self.position_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // 2, 4)  # [x_mean, x_std, y_mean, y_std]\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_types, input_positions, input_mask):\n",
    "        # Embeddings\n",
    "        type_emb = self.type_embedding(input_types)  # [B, L, D]\n",
    "        pos_emb = self.position_encoder(input_positions)  # [B, L, D]\n",
    "        \n",
    "        # Mask positions (zero out Shot/GOAL)\n",
    "        pos_emb = pos_emb * input_mask.unsqueeze(-1).float()\n",
    "        \n",
    "        # Combine\n",
    "        hidden = type_emb + pos_emb  # [B, L, D]\n",
    "        \n",
    "        # Causal mask\n",
    "        seq_len = hidden.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(hidden.device)\n",
    "        \n",
    "        # Transformer\n",
    "        hidden = self.transformer(hidden, mask=causal_mask)  # [B, L, D]\n",
    "        \n",
    "        # Predictions\n",
    "        type_logits = self.type_head(hidden)  # [B, L, n_types]\n",
    "        pos_pred = self.position_head(hidden)  # [B, L, 4]\n",
    "        \n",
    "        x_mean, x_std, y_mean, y_std = pos_pred.chunk(4, dim=-1)\n",
    "        x_std = torch.exp(x_std)  # positive\n",
    "        y_std = torch.exp(y_std)\n",
    "        \n",
    "        return type_logits, x_mean.squeeze(-1), x_std.squeeze(-1), y_mean.squeeze(-1), y_std.squeeze(-1)\n",
    "\n",
    "# Create model\n",
    "model = ContinuousEventModel(\n",
    "    n_types=len(type_vocab),\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_layers=12\n",
    ").cuda()\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4aa1d49-723d-4efe-811f-172278ccc6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes:\n",
      "  type_logits: torch.Size([16, 13, 7])\n",
      "  x_mean: torch.Size([16, 13])\n",
      "  x_std: torch.Size([16, 13])\n",
      "  y_mean: torch.Size([16, 13])\n",
      "  y_std: torch.Size([16, 13])\n",
      "\n",
      "Sample predictions:\n",
      "  x_std range: [0.78, 1.36]\n",
      "  y_std range: [0.95, 1.53]\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "batch = next(iter(train_loader))\n",
    "input_types = batch['input_types'].cuda()\n",
    "input_positions = batch['input_positions'].cuda()\n",
    "input_mask = batch['input_mask'].cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    type_logits, x_mean, x_std, y_mean, y_std = model(input_types, input_positions, input_mask)\n",
    "\n",
    "print(\"Output shapes:\")\n",
    "print(f\"  type_logits: {type_logits.shape}\")\n",
    "print(f\"  x_mean: {x_mean.shape}\")\n",
    "print(f\"  x_std: {x_std.shape}\")\n",
    "print(f\"  y_mean: {y_mean.shape}\")\n",
    "print(f\"  y_std: {y_std.shape}\")\n",
    "\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(f\"  x_std range: [{x_std.min():.2f}, {x_std.max():.2f}]\")\n",
    "print(f\"  y_std range: [{y_std.min():.2f}, {y_std.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9da1ad6a-8a96-4c31-9949-0b10d700bb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses:\n",
      "  type: 1.9671\n",
      "  x:    1.2997\n",
      "  y:    1.2777\n",
      "  total: 4.5445\n"
     ]
    }
   ],
   "source": [
    "# === CELL 7: Loss ===\n",
    "def gaussian_nll_loss(mean, std, target, mask):\n",
    "    \"\"\"Gaussian negative log-likelihood\"\"\"\n",
    "    loss = 0.5 * torch.log(2 * math.pi * std**2) + (target - mean)**2 / (2 * std**2)\n",
    "    loss = loss * mask.float()  # apply mask\n",
    "    return loss.sum() / mask.sum().clamp(min=1)\n",
    "\n",
    "def compute_loss(model, batch):\n",
    "    input_types = batch['input_types'].cuda()\n",
    "    input_positions = batch['input_positions'].cuda()\n",
    "    input_mask = batch['input_mask'].cuda()\n",
    "    target_types = batch['target_types'].cuda()\n",
    "    target_positions = batch['target_positions'].cuda()\n",
    "    target_mask = batch['target_mask'].cuda()\n",
    "    \n",
    "    # Forward\n",
    "    type_logits, x_mean, x_std, y_mean, y_std = model(input_types, input_positions, input_mask)\n",
    "    \n",
    "    # Type loss (cross entropy)\n",
    "    type_loss = nn.functional.cross_entropy(\n",
    "        type_logits.reshape(-1, type_logits.size(-1)),\n",
    "        target_types.reshape(-1),\n",
    "        ignore_index=-100\n",
    "    )\n",
    "    \n",
    "    # Position losses (Gaussian NLL, masked)\n",
    "    x_loss = gaussian_nll_loss(x_mean, x_std, target_positions[..., 0], target_mask)\n",
    "    y_loss = gaussian_nll_loss(y_mean, y_std, target_positions[..., 1], target_mask)\n",
    "    \n",
    "    # Combined (equal weights for now)\n",
    "    total_loss = type_loss + x_loss + y_loss\n",
    "    \n",
    "    return total_loss, type_loss, x_loss, y_loss\n",
    "\n",
    "# Test\n",
    "batch = next(iter(train_loader))\n",
    "total_loss, type_loss, x_loss, y_loss = compute_loss(model, batch)\n",
    "print(f\"Losses:\")\n",
    "print(f\"  type: {type_loss.item():.4f}\")\n",
    "print(f\"  x:    {x_loss.item():.4f}\")\n",
    "print(f\"  y:    {y_loss.item():.4f}\")\n",
    "print(f\"  total: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "127b938f-8a28-404a-bae0-2f25af1ebd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epochs: 15\n",
      "\n",
      "Epoch 1/15 | Batch 100/951 | Loss: 1.3628\n",
      "Epoch 1/15 | Batch 200/951 | Loss: 1.2014\n",
      "Epoch 1/15 | Batch 300/951 | Loss: 1.0625\n",
      "Epoch 1/15 | Batch 400/951 | Loss: 0.8580\n",
      "Epoch 1/15 | Batch 500/951 | Loss: 0.6844\n",
      "Epoch 1/15 | Batch 600/951 | Loss: 0.5559\n",
      "Epoch 1/15 | Batch 700/951 | Loss: 0.4534\n",
      "Epoch 1/15 | Batch 800/951 | Loss: 0.3658\n",
      "Epoch 1/15 | Batch 900/951 | Loss: 0.2940\n",
      "âœ… Epoch 1 | Loss: 0.2596 (type: 0.8325, x: -0.4102, y: -0.1627)\n",
      "\n",
      "Epoch 2/15 | Batch 100/951 | Loss: -0.2665\n",
      "Epoch 2/15 | Batch 200/951 | Loss: -0.2861\n",
      "Epoch 2/15 | Batch 300/951 | Loss: -0.2893\n",
      "Epoch 2/15 | Batch 400/951 | Loss: -0.2981\n",
      "Epoch 2/15 | Batch 500/951 | Loss: -0.3179\n",
      "Epoch 2/15 | Batch 600/951 | Loss: -0.3246\n",
      "Epoch 2/15 | Batch 700/951 | Loss: -0.3322\n",
      "Epoch 2/15 | Batch 800/951 | Loss: -0.3374\n",
      "Epoch 2/15 | Batch 900/951 | Loss: -0.3361\n",
      "âœ… Epoch 2 | Loss: -0.3363 (type: 0.7489, x: -0.6578, y: -0.4274)\n",
      "\n",
      "Epoch 3/15 | Batch 100/951 | Loss: -0.3985\n",
      "Epoch 3/15 | Batch 200/951 | Loss: -0.4007\n",
      "Epoch 3/15 | Batch 300/951 | Loss: -0.4056\n",
      "Epoch 3/15 | Batch 400/951 | Loss: -0.4029\n",
      "Epoch 3/15 | Batch 500/951 | Loss: -0.4069\n",
      "Epoch 3/15 | Batch 600/951 | Loss: -0.4089\n",
      "Epoch 3/15 | Batch 700/951 | Loss: -0.4135\n",
      "Epoch 3/15 | Batch 800/951 | Loss: -0.4113\n",
      "Epoch 3/15 | Batch 900/951 | Loss: -0.4175\n",
      "âœ… Epoch 3 | Loss: -0.4166 (type: 0.7329, x: -0.6945, y: -0.4550)\n",
      "\n",
      "Epoch 4/15 | Batch 100/951 | Loss: -0.4808\n",
      "Epoch 4/15 | Batch 200/951 | Loss: -0.4757\n",
      "Epoch 4/15 | Batch 300/951 | Loss: -0.4526\n",
      "Epoch 4/15 | Batch 400/951 | Loss: -0.4392\n",
      "Epoch 4/15 | Batch 500/951 | Loss: -0.4518\n",
      "Epoch 4/15 | Batch 600/951 | Loss: -0.4530\n",
      "Epoch 4/15 | Batch 700/951 | Loss: -0.4540\n",
      "Epoch 4/15 | Batch 800/951 | Loss: -0.4619\n",
      "Epoch 4/15 | Batch 900/951 | Loss: -0.4609\n",
      "âœ… Epoch 4 | Loss: -0.4635 (type: 0.7210, x: -0.7143, y: -0.4702)\n",
      "\n",
      "Epoch 5/15 | Batch 100/951 | Loss: -0.4674\n",
      "Epoch 5/15 | Batch 200/951 | Loss: -0.4693\n",
      "Epoch 5/15 | Batch 300/951 | Loss: -0.4819\n",
      "Epoch 5/15 | Batch 400/951 | Loss: -0.4779\n",
      "Epoch 5/15 | Batch 500/951 | Loss: -0.4752\n",
      "Epoch 5/15 | Batch 600/951 | Loss: -0.4822\n",
      "Epoch 5/15 | Batch 700/951 | Loss: -0.4860\n",
      "Epoch 5/15 | Batch 800/951 | Loss: -0.4861\n",
      "Epoch 5/15 | Batch 900/951 | Loss: -0.4813\n",
      "âœ… Epoch 5 | Loss: -0.4794 (type: 0.7182, x: -0.7159, y: -0.4817)\n",
      "\n",
      "Epoch 6/15 | Batch 100/951 | Loss: -0.4559\n",
      "Epoch 6/15 | Batch 200/951 | Loss: -0.4679\n",
      "Epoch 6/15 | Batch 300/951 | Loss: -0.4837\n",
      "Epoch 6/15 | Batch 400/951 | Loss: -0.4932\n",
      "Epoch 6/15 | Batch 500/951 | Loss: -0.4923\n",
      "Epoch 6/15 | Batch 600/951 | Loss: -0.4976\n",
      "Epoch 6/15 | Batch 700/951 | Loss: -0.5017\n",
      "Epoch 6/15 | Batch 800/951 | Loss: -0.4949\n",
      "Epoch 6/15 | Batch 900/951 | Loss: -0.4905\n",
      "âœ… Epoch 6 | Loss: -0.4914 (type: 0.7148, x: -0.7222, y: -0.4840)\n",
      "\n",
      "Epoch 7/15 | Batch 100/951 | Loss: -0.5480\n",
      "Epoch 7/15 | Batch 200/951 | Loss: -0.5409\n",
      "Epoch 7/15 | Batch 300/951 | Loss: -0.5354\n",
      "Epoch 7/15 | Batch 400/951 | Loss: -0.5323\n",
      "Epoch 7/15 | Batch 500/951 | Loss: -0.5317\n",
      "Epoch 7/15 | Batch 600/951 | Loss: -0.5249\n",
      "Epoch 7/15 | Batch 700/951 | Loss: -0.5243\n",
      "Epoch 7/15 | Batch 800/951 | Loss: -0.5237\n",
      "Epoch 7/15 | Batch 900/951 | Loss: -0.5219\n",
      "âœ… Epoch 7 | Loss: -0.5228 (type: 0.7044, x: -0.7351, y: -0.4921)\n",
      "\n",
      "Epoch 8/15 | Batch 100/951 | Loss: -0.5612\n",
      "Epoch 8/15 | Batch 200/951 | Loss: -0.5388\n",
      "Epoch 8/15 | Batch 300/951 | Loss: -0.5381\n",
      "Epoch 8/15 | Batch 400/951 | Loss: -0.5326\n",
      "Epoch 8/15 | Batch 500/951 | Loss: -0.5290\n",
      "Epoch 8/15 | Batch 600/951 | Loss: -0.5306\n",
      "Epoch 8/15 | Batch 700/951 | Loss: -0.5316\n",
      "Epoch 8/15 | Batch 800/951 | Loss: -0.5320\n",
      "Epoch 8/15 | Batch 900/951 | Loss: -0.5377\n",
      "âœ… Epoch 8 | Loss: -0.5413 (type: 0.7016, x: -0.7420, y: -0.5009)\n",
      "\n",
      "Epoch 9/15 | Batch 100/951 | Loss: -0.5617\n",
      "Epoch 9/15 | Batch 200/951 | Loss: -0.5373\n",
      "Epoch 9/15 | Batch 300/951 | Loss: -0.5358\n",
      "Epoch 9/15 | Batch 400/951 | Loss: -0.5470\n",
      "Epoch 9/15 | Batch 500/951 | Loss: -0.5447\n",
      "Epoch 9/15 | Batch 600/951 | Loss: -0.5415\n",
      "Epoch 9/15 | Batch 700/951 | Loss: -0.5417\n",
      "Epoch 9/15 | Batch 800/951 | Loss: -0.5418\n",
      "Epoch 9/15 | Batch 900/951 | Loss: -0.5410\n",
      "âœ… Epoch 9 | Loss: -0.5452 (type: 0.6995, x: -0.7425, y: -0.5022)\n",
      "\n",
      "Epoch 10/15 | Batch 100/951 | Loss: -0.5780\n",
      "Epoch 10/15 | Batch 200/951 | Loss: -0.5465\n",
      "Epoch 10/15 | Batch 300/951 | Loss: -0.5494\n",
      "Epoch 10/15 | Batch 400/951 | Loss: -0.5701\n",
      "Epoch 10/15 | Batch 500/951 | Loss: -0.5623\n",
      "Epoch 10/15 | Batch 600/951 | Loss: -0.5704\n",
      "Epoch 10/15 | Batch 700/951 | Loss: -0.5714\n",
      "Epoch 10/15 | Batch 800/951 | Loss: -0.5687\n",
      "Epoch 10/15 | Batch 900/951 | Loss: -0.5654\n",
      "âœ… Epoch 10 | Loss: -0.5646 (type: 0.6946, x: -0.7497, y: -0.5095)\n",
      "\n",
      "Epoch 11/15 | Batch 100/951 | Loss: -0.5480\n",
      "Epoch 11/15 | Batch 200/951 | Loss: -0.5712\n",
      "Epoch 11/15 | Batch 300/951 | Loss: -0.5794\n",
      "Epoch 11/15 | Batch 400/951 | Loss: -0.5876\n",
      "Epoch 11/15 | Batch 500/951 | Loss: -0.5866\n",
      "Epoch 11/15 | Batch 600/951 | Loss: -0.5834\n",
      "Epoch 11/15 | Batch 700/951 | Loss: -0.5863\n",
      "Epoch 11/15 | Batch 800/951 | Loss: -0.5814\n",
      "Epoch 11/15 | Batch 900/951 | Loss: -0.5823\n",
      "âœ… Epoch 11 | Loss: -0.5841 (type: 0.6902, x: -0.7603, y: -0.5140)\n",
      "\n",
      "Epoch 12/15 | Batch 100/951 | Loss: -0.5700\n",
      "Epoch 12/15 | Batch 200/951 | Loss: -0.5831\n",
      "Epoch 12/15 | Batch 300/951 | Loss: -0.5992\n",
      "Epoch 12/15 | Batch 400/951 | Loss: -0.5984\n",
      "Epoch 12/15 | Batch 500/951 | Loss: -0.5903\n",
      "Epoch 12/15 | Batch 600/951 | Loss: -0.5901\n",
      "Epoch 12/15 | Batch 700/951 | Loss: -0.5884\n",
      "Epoch 12/15 | Batch 800/951 | Loss: -0.5939\n",
      "Epoch 12/15 | Batch 900/951 | Loss: -0.5910\n",
      "âœ… Epoch 12 | Loss: -0.5891 (type: 0.6884, x: -0.7599, y: -0.5176)\n",
      "\n",
      "Epoch 13/15 | Batch 100/951 | Loss: -0.6009\n",
      "Epoch 13/15 | Batch 200/951 | Loss: -0.5894\n",
      "Epoch 13/15 | Batch 300/951 | Loss: -0.5987\n",
      "Epoch 13/15 | Batch 400/951 | Loss: -0.5995\n",
      "Epoch 13/15 | Batch 500/951 | Loss: -0.5909\n",
      "Epoch 13/15 | Batch 600/951 | Loss: -0.5941\n",
      "Epoch 13/15 | Batch 700/951 | Loss: -0.5901\n",
      "Epoch 13/15 | Batch 800/951 | Loss: -0.5953\n",
      "Epoch 13/15 | Batch 900/951 | Loss: -0.5994\n",
      "âœ… Epoch 13 | Loss: -0.5993 (type: 0.6861, x: -0.7662, y: -0.5191)\n",
      "\n",
      "Epoch 14/15 | Batch 100/951 | Loss: -0.6525\n",
      "Epoch 14/15 | Batch 200/951 | Loss: -0.6163\n",
      "Epoch 14/15 | Batch 300/951 | Loss: -0.6096\n",
      "Epoch 14/15 | Batch 400/951 | Loss: -0.6221\n",
      "Epoch 14/15 | Batch 500/951 | Loss: -0.6269\n",
      "Epoch 14/15 | Batch 600/951 | Loss: -0.6234\n",
      "Epoch 14/15 | Batch 700/951 | Loss: -0.6159\n",
      "Epoch 14/15 | Batch 800/951 | Loss: -0.6184\n",
      "Epoch 14/15 | Batch 900/951 | Loss: -0.6185\n",
      "âœ… Epoch 14 | Loss: -0.6148 (type: 0.6836, x: -0.7718, y: -0.5266)\n",
      "\n",
      "Epoch 15/15 | Batch 100/951 | Loss: -0.6159\n",
      "Epoch 15/15 | Batch 200/951 | Loss: -0.6034\n",
      "Epoch 15/15 | Batch 300/951 | Loss: -0.6006\n",
      "Epoch 15/15 | Batch 400/951 | Loss: -0.6077\n",
      "Epoch 15/15 | Batch 500/951 | Loss: -0.6087\n",
      "Epoch 15/15 | Batch 600/951 | Loss: -0.6146\n",
      "Epoch 15/15 | Batch 700/951 | Loss: -0.6122\n",
      "Epoch 15/15 | Batch 800/951 | Loss: -0.6111\n",
      "Epoch 15/15 | Batch 900/951 | Loss: -0.6138\n",
      "âœ… Epoch 15 | Loss: -0.6135 (type: 0.6835, x: -0.7709, y: -0.5261)\n",
      "\n",
      "ðŸŽ‰ Training finished!\n"
     ]
    }
   ],
   "source": [
    "# === CELL 8: Training ===\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "epochs = 15\n",
    "\n",
    "print(f\"Starting training...\")\n",
    "print(f\"Epochs: {epochs}\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_type_loss = 0\n",
    "    total_x_loss = 0\n",
    "    total_y_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        loss, type_loss, x_loss, y_loss = compute_loss(model, batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_type_loss += type_loss.item()\n",
    "        total_x_loss += x_loss.item()\n",
    "        total_y_loss += y_loss.item()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Batch {batch_idx+1}/{len(train_loader)} | Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Epoch summary\n",
    "    n = len(train_loader)\n",
    "    print(f\"âœ… Epoch {epoch+1} | Loss: {total_loss/n:.4f} (type: {total_type_loss/n:.4f}, x: {total_x_loss/n:.4f}, y: {total_y_loss/n:.4f})\\n\")\n",
    "\n",
    "print(\"ðŸŽ‰ Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d112e786-77fb-4400-b077-a534492a3fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "  Total: -0.6098\n",
      "  Type:  0.6743\n",
      "  X:     -0.7523\n",
      "  Y:     -0.5318\n"
     ]
    }
   ],
   "source": [
    "# === CELL 9: Validation ===\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "val_type_loss = 0\n",
    "val_x_loss = 0\n",
    "val_y_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        loss, type_loss, x_loss, y_loss = compute_loss(model, batch)\n",
    "        val_loss += loss.item()\n",
    "        val_type_loss += type_loss.item()\n",
    "        val_x_loss += x_loss.item()\n",
    "        val_y_loss += y_loss.item()\n",
    "\n",
    "n = len(val_loader)\n",
    "print(f\"Validation Results:\")\n",
    "print(f\"  Total: {val_loss/n:.4f}\")\n",
    "print(f\"  Type:  {val_type_loss/n:.4f}\")\n",
    "print(f\"  X:     {val_x_loss/n:.4f}\")\n",
    "print(f\"  Y:     {val_y_loss/n:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82fdb3c7-bc6f-4953-b584-e1f21c177bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'artifacts/continuous_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1603db0-ba4d-4e75-b7b5-aec515818a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 10: Monte Carlo xT ===\n",
    "def calculate_xT_continuous(model, start_events, type_vocab, id_to_type,\n",
    "                            n_rollouts=1000, n_steps=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Monte Carlo xT dla continuous model.\n",
    "    start_events: lista dict [{'type': 'START', 'end_x': 85.0, 'end_y': 15.0}, ...]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    goal_count = 0\n",
    "    goal_id = type_vocab['GOAL']\n",
    "    pad_id = type_vocab['<pad>']\n",
    "    \n",
    "    for _ in range(n_rollouts):\n",
    "        # Skopiuj start sequence\n",
    "        current_types = [type_vocab[e['type']] for e in start_events]\n",
    "        current_positions = [[e['end_x']/120.0, e['end_y']/80.0] for e in start_events]\n",
    "        current_masks = [e['end_x'] is not None for e in start_events]\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "            # Przygotuj input (ostatnie 13)\n",
    "            inp_types = current_types[-13:] if len(current_types) > 13 else current_types\n",
    "            inp_pos = current_positions[-13:] if len(current_positions) > 13 else current_positions\n",
    "            inp_mask = current_masks[-13:] if len(current_masks) > 13 else current_masks\n",
    "            \n",
    "            # Pad\n",
    "            pad_len = 13 - len(inp_types)\n",
    "            inp_types = inp_types + [pad_id] * pad_len\n",
    "            inp_pos = inp_pos + [[0.0, 0.0]] * pad_len\n",
    "            inp_mask = inp_mask + [False] * pad_len\n",
    "            \n",
    "            # Tensors\n",
    "            inp_types_t = torch.tensor([inp_types], dtype=torch.long).to(device)\n",
    "            inp_pos_t = torch.tensor([inp_pos], dtype=torch.float).to(device)\n",
    "            inp_mask_t = torch.tensor([inp_mask], dtype=torch.bool).to(device)\n",
    "            \n",
    "            # Forward\n",
    "            with torch.no_grad():\n",
    "                type_logits, x_mean, x_std, y_mean, y_std = model(inp_types_t, inp_pos_t, inp_mask_t)\n",
    "                \n",
    "                # Ostatnia pozycja w sekwencji\n",
    "                idx = len(current_types) - 1\n",
    "                if idx >= 13:\n",
    "                    idx = 12\n",
    "                \n",
    "                # Sample type\n",
    "                probs = torch.softmax(type_logits[0, idx], dim=0)\n",
    "                next_type = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                # Sample position (Gaussian)\n",
    "                if next_type not in [type_vocab['Shot'], goal_id, type_vocab['NO_GOAL']]:\n",
    "                    x_sample = torch.normal(x_mean[0, idx], x_std[0, idx]).item()\n",
    "                    y_sample = torch.normal(y_mean[0, idx], y_std[0, idx]).item()\n",
    "                    # Clip to [0, 1]\n",
    "                    x_sample = max(0.0, min(1.0, x_sample))\n",
    "                    y_sample = max(0.0, min(1.0, y_sample))\n",
    "                    has_pos = True\n",
    "                else:\n",
    "                    x_sample, y_sample = 0.0, 0.0\n",
    "                    has_pos = False\n",
    "            \n",
    "            # Append\n",
    "            current_types.append(next_type)\n",
    "            current_positions.append([x_sample, y_sample])\n",
    "            current_masks.append(has_pos)\n",
    "            \n",
    "            # Check goal\n",
    "            if next_type == goal_id:\n",
    "                goal_count += 1\n",
    "                break\n",
    "    \n",
    "    return goal_count / n_rollouts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "933e1539-0605-4594-a0d5-18b47f14b491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xT Low zone:  0.80%\n",
      "xT High zone: 33.20%\n",
      "Sanity: True\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "start_low = [\n",
    "    {'type': 'START', 'end_x': 40.0, 'end_y': 10.0},\n",
    "    {'type': 'Pass', 'end_x': 45.0, 'end_y': 15.0}\n",
    "]\n",
    "start_high = [\n",
    "    {'type': 'START', 'end_x': 100.0, 'end_y': 35.0},\n",
    "    {'type': 'Pass', 'end_x': 105.0, 'end_y': 40.0}\n",
    "]\n",
    "\n",
    "xT_low = calculate_xT_continuous(model, start_low, type_vocab, id_to_type, n_rollouts=500)\n",
    "xT_high = calculate_xT_continuous(model, start_high, type_vocab, id_to_type, n_rollouts=500)\n",
    "\n",
    "print(f\"xT Low zone:  {xT_low*100:.2f}%\")\n",
    "print(f\"xT High zone: {xT_high*100:.2f}%\")\n",
    "print(f\"Sanity: {xT_low < xT_high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bc13c9c-d6ff-4fb1-a2cb-6b180ce45c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 3\n",
      "Max: 14\n",
      "Mean: 7.6\n",
      "Median: 7\n",
      "\n",
      "Dystrybucja:\n",
      "  < 5:  1261\n",
      "  5-10: 1245\n",
      "  >=10: 1298\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# SprawdÅº dÅ‚ugoÅ›ci sekwencji w val_df\n",
    "val_lengths = val_df['sequence_length'].values\n",
    "print(f\"Min: {val_lengths.min()}\")\n",
    "print(f\"Max: {val_lengths.max()}\")\n",
    "print(f\"Mean: {val_lengths.mean():.1f}\")\n",
    "print(f\"Median: {np.median(val_lengths):.0f}\")\n",
    "print(f\"\\nDystrybucja:\")\n",
    "print(f\"  < 5:  {(val_lengths < 5).sum()}\")\n",
    "print(f\"  5-10: {((val_lengths >= 5) & (val_lengths < 10)).sum()}\")\n",
    "print(f\"  >=10: {(val_lengths >= 10).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd7b2b3e-35cb-4c93-a2b0-9bad8600b285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 32/100\n",
      "Processed 64/100\n",
      "Processed 96/100\n",
      "Processed 100/100\n",
      "ROC-AUC: 0.3830\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "\n",
    "def calculate_xT_batched(model, start_events_batch, type_vocab, id_to_type,\n",
    "                         n_rollouts=100, n_steps=5, device='cuda'):\n",
    "    model.eval()\n",
    "    batch_size = len(start_events_batch)\n",
    "    goal_id = type_vocab['GOAL']\n",
    "    pad_id = type_vocab['<pad>']\n",
    "    \n",
    "    all_types = []\n",
    "    all_positions = []\n",
    "    all_masks = []\n",
    "    \n",
    "    for start_events in start_events_batch:\n",
    "        for _ in range(n_rollouts):\n",
    "            types = [type_vocab[e['type']] for e in start_events]\n",
    "            positions = [\n",
    "                [e['end_x']/120.0 if e['end_x'] is not None else 0.0,\n",
    "                 e['end_y']/80.0 if e['end_y'] is not None else 0.0] \n",
    "                for e in start_events\n",
    "            ]\n",
    "            masks = [e['end_x'] is not None for e in start_events]\n",
    "            all_types.append(types)\n",
    "            all_positions.append(positions)\n",
    "            all_masks.append(masks)\n",
    "    \n",
    "    goal_counts = torch.zeros(batch_size, device=device)\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        padded_types = []\n",
    "        padded_positions = []\n",
    "        padded_masks = []\n",
    "        \n",
    "        for types, positions, masks in zip(all_types, all_positions, all_masks):\n",
    "            inp = types[-13:] if len(types) > 13 else types\n",
    "            pos = positions[-13:] if len(positions) > 13 else positions\n",
    "            msk = masks[-13:] if len(masks) > 13 else masks\n",
    "            \n",
    "            pad_len = 13 - len(inp)\n",
    "            inp = inp + [pad_id] * pad_len\n",
    "            pos = pos + [[0.0, 0.0]] * pad_len\n",
    "            msk = msk + [False] * pad_len\n",
    "            \n",
    "            padded_types.append(inp)\n",
    "            padded_positions.append(pos)\n",
    "            padded_masks.append(msk)\n",
    "        \n",
    "        types_t = torch.tensor(padded_types, dtype=torch.long, device=device)\n",
    "        pos_t = torch.tensor(padded_positions, dtype=torch.float, device=device)\n",
    "        mask_t = torch.tensor(padded_masks, dtype=torch.bool, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            type_logits, x_mean, x_std, y_mean, y_std = model(types_t, pos_t, mask_t)\n",
    "            \n",
    "            seq_lens = torch.tensor([min(len(t)-1, 12) for t in all_types], device=device)\n",
    "            batch_idx = torch.arange(len(all_types), device=device)\n",
    "            \n",
    "            probs = torch.softmax(type_logits[batch_idx, seq_lens], dim=-1)\n",
    "            next_types = torch.multinomial(probs, 1).squeeze(-1)\n",
    "            \n",
    "            x_samples = torch.normal(x_mean[batch_idx, seq_lens], x_std[batch_idx, seq_lens]).clamp(0, 1)\n",
    "            y_samples = torch.normal(y_mean[batch_idx, seq_lens], y_std[batch_idx, seq_lens]).clamp(0, 1)\n",
    "        \n",
    "        for i, next_type in enumerate(next_types):\n",
    "            all_types[i].append(next_type.item())\n",
    "            if next_type not in [type_vocab['Shot'], goal_id, type_vocab['NO_GOAL']]:\n",
    "                all_positions[i].append([x_samples[i].item(), y_samples[i].item()])\n",
    "                all_masks[i].append(True)\n",
    "            else:\n",
    "                all_positions[i].append([0.0, 0.0])\n",
    "                all_masks[i].append(False)\n",
    "        \n",
    "        goals_mask = (next_types == goal_id).view(batch_size, n_rollouts)\n",
    "        goal_counts += goals_mask.sum(dim=1).float()\n",
    "    \n",
    "    return (goal_counts / n_rollouts).cpu().numpy()\n",
    "\n",
    "\n",
    "def evaluate_xT_on_dataset(model, df, type_vocab, id_to_type, n_rollouts=100, batch_size=32):\n",
    "    predictions = []\n",
    "    labels = df['goal'].values\n",
    "    \n",
    "    for batch_start in range(0, len(df), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(df))\n",
    "        \n",
    "        # Przygotuj batch startÃ³w\n",
    "        start_events_batch = [df['events'].iloc[i][:2] for i in range(batch_start, batch_end)]\n",
    "        \n",
    "        # Batched Monte Carlo\n",
    "        xT_batch = calculate_xT_batched(model, start_events_batch, type_vocab, id_to_type, n_rollouts)\n",
    "        predictions.extend(xT_batch)\n",
    "        \n",
    "        print(f\"Processed {batch_end}/{len(df)}\")\n",
    "    \n",
    "    auc = roc_auc_score(labels, predictions)\n",
    "    return auc, predictions, labels\n",
    "\n",
    "# Test\n",
    "auc, preds, labs = evaluate_xT_on_dataset(model, val_df.iloc[:100], type_vocab, id_to_type, \n",
    "                                          n_rollouts=100, batch_size=32)\n",
    "print(f\"ROC-AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80378600-0346-41f4-8846-e5ab8f601101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 32/3804\n",
      "Processed 64/3804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m auc, preds, labs = \u001b[43mevaluate_xT_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_to_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                          \u001b[49m\u001b[43mn_rollouts\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mROC-AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mevaluate_xT_on_dataset\u001b[39m\u001b[34m(model, df, type_vocab, id_to_type, n_rollouts, batch_size)\u001b[39m\n\u001b[32m     87\u001b[39m start_events_batch = [df[\u001b[33m'\u001b[39m\u001b[33mevents\u001b[39m\u001b[33m'\u001b[39m].iloc[i][:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_start, batch_end)]\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Batched Monte Carlo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m xT_batch = \u001b[43mcalculate_xT_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_events_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_to_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m predictions.extend(xT_batch)\n\u001b[32m     93\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mcalculate_xT_batched\u001b[39m\u001b[34m(model, start_events_batch, type_vocab, id_to_type, n_rollouts, n_steps, device)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     53\u001b[39m     type_logits, x_mean, x_std, y_mean, y_std = model(types_t, pos_t, mask_t)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     seq_lens = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_types\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     batch_idx = torch.arange(\u001b[38;5;28mlen\u001b[39m(all_types), device=device)\n\u001b[32m     58\u001b[39m     probs = torch.softmax(type_logits[batch_idx, seq_lens], dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "auc, preds, labs = evaluate_xT_on_dataset(model, val_df, type_vocab, id_to_type, \n",
    "                                          n_rollouts=500, batch_size=32)\n",
    "print(f\"ROC-AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cf7e1-b419-4692-925b-b43e230b38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº None w pierwszych 100 sekwencjach\n",
    "for i in range(300):\n",
    "    events = val_df['events'].iloc[i][:2]\n",
    "    for j, e in enumerate(events):\n",
    "        if e['end_x'] is None:\n",
    "            print(f\"Seq {i}, Event {j}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
