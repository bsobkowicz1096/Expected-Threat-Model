{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccbbd01-6539-4292-ad19-b32040eed193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad82284-50a0-4c40-b77b-888d32c6d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# === CELL 1: Imports ===\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from peft import LoraConfig, get_peft_model, TaskType  # ‚¨ÖÔ∏è DODAJ TO\n",
    "\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1b4304-033c-4f9a-a413-cab7b9aa1718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences: 18,988\n",
      "Goals: 951 (5.0%)\n",
      "Vocab size: 1272\n",
      "\n",
      "Sample sequence: ['START_LOC_20_5' 'Pass_LOC_5_25' 'Carry_LOC_5_35' 'Pass_LOC_45_65'\n",
      " 'Pass_LOC_35_55' 'Carry_LOC_35_55' 'Carry_LOC_50_60' 'Pass_LOC_95_45'\n",
      " 'Carry_LOC_90_45' 'Pass_LOC_105_35' 'Carry_LOC_115_35' 'SHOT' 'GOAL']\n"
     ]
    }
   ],
   "source": [
    "# Sequences\n",
    "df = pd.read_parquet('data/sequences_balanced_1.parquet')\n",
    "print(f\"Sequences: {len(df):,}\")\n",
    "print(f\"Goals: {df['goal'].sum()} ({df['goal'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Vocabulary\n",
    "with open('data/vocab_1.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "with open('data/id_to_token_1.json', 'r') as f:\n",
    "    id_to_token = json.load(f)\n",
    "    id_to_token = {int(k): v for k, v in id_to_token.items()}\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"\\nSample sequence: {df['full_sequence'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b49494d1-3ad8-431b-ac93-c6872e17cd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 18988\n"
     ]
    }
   ],
   "source": [
    "class CausalLMDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_length=14):\n",
    "        self.sequences = df['full_sequence'].tolist()\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "        self.pad_id = vocab['<pad>']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.sequences[idx]\n",
    "        ids = [self.vocab[token] for token in tokens]\n",
    "        \n",
    "        # Shifting\n",
    "        input_ids = ids[:-1]\n",
    "        labels = ids[1:]\n",
    "        \n",
    "        # Padding\n",
    "        pad_length = self.max_length - 1 - len(input_ids)\n",
    "        input_ids = input_ids + [self.pad_id] * pad_length\n",
    "        labels = labels + [-100] * pad_length\n",
    "        \n",
    "        # Truncate\n",
    "        input_ids = input_ids[:self.max_length - 1]\n",
    "        labels = labels[:self.max_length - 1]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "dataset = CausalLMDataset(df, vocab)\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dedf5249-7a68-4dc3-b341-6b424e512caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET SPLIT ===\n",
      "Total:    18,988\n",
      "Train:    15,190 (80.0%)\n",
      "Val:      3,798 (20.0%)\n",
      "\n",
      "=== GOAL DISTRIBUTION ===\n",
      "Train goals: 761 (5.0%)\n",
      "Val goals:   190 (5.0%)\n"
     ]
    }
   ],
   "source": [
    "# === CELL 4: Stratified Train/Val Split ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split zachowujƒÖc proporcje goali\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['goal'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"=== DATASET SPLIT ===\")\n",
    "print(f\"Total:    {len(df):,}\")\n",
    "print(f\"Train:    {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Val:      {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== GOAL DISTRIBUTION ===\")\n",
    "print(f\"Train goals: {train_df['goal'].sum()} ({train_df['goal'].mean()*100:.1f}%)\")\n",
    "print(f\"Val goals:   {val_df['goal'].sum()} ({val_df['goal'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c268ff-7538-4101-9f49-865f4777b02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATASETS ===\n",
      "Train dataset: 15,190\n",
      "Val dataset:   3,798\n",
      "\n",
      "Train batches: 950\n",
      "Val batches:   238\n",
      "\n",
      "Batch shapes:\n",
      "  input_ids: torch.Size([16, 13])\n",
      "  labels:    torch.Size([16, 13])\n"
     ]
    }
   ],
   "source": [
    "# Create datasets z podzielonych DataFrames\n",
    "train_dataset = CausalLMDataset(train_df, vocab)\n",
    "val_dataset = CausalLMDataset(val_df, vocab)\n",
    "\n",
    "print(f\"\\n=== DATASETS ===\")\n",
    "print(f\"Train dataset: {len(train_dataset):,}\")\n",
    "print(f\"Val dataset:   {len(val_dataset):,}\")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")\n",
    "\n",
    "# Test\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  input_ids: {batch['input_ids'].shape}\")\n",
    "print(f\"  labels:    {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ded007-6ca2-4c4c-b126-83895f50ea5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created!\n",
      "Parameters: 86,043,648\n",
      "Trainable: 86,043,648\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# === MODEL CONFIG ===\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(vocab),        # nasze tokeny (853)\n",
    "    n_positions=14,                # max sequence length\n",
    "    n_ctx=14,                      # context window\n",
    "    n_embd=768,                    # embedding dimension (GPT-2 small)\n",
    "    n_layer=12,                    # transformer layers\n",
    "    n_head=12                      # attention heads\n",
    ")\n",
    "\n",
    "# === CREATE MODEL ===\n",
    "model = GPT2LMHeadModel(config)\n",
    "model = model.cuda()  # przenie≈õ na GPU\n",
    "\n",
    "print(f\"Model created!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4aa1d49-723d-4efe-811f-172278ccc6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW\n",
      "Learning rate: 0.0005\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# === OPTIMIZER ===\n",
    "learning_rate = 5e-4\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1ad6a-8a96-4c31-9949-0b10d700bb4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epochs: 3\n",
      "Device: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Batch 50/950 | Loss: 6.4649\n",
      "Epoch 1/3 | Batch 100/950 | Loss: 6.2264\n",
      "Epoch 1/3 | Batch 150/950 | Loss: 6.1112\n",
      "Epoch 1/3 | Batch 200/950 | Loss: 6.0555\n",
      "Epoch 1/3 | Batch 250/950 | Loss: 6.0279\n",
      "Epoch 1/3 | Batch 300/950 | Loss: 5.9901\n",
      "Epoch 1/3 | Batch 350/950 | Loss: 5.9583\n",
      "Epoch 1/3 | Batch 400/950 | Loss: 5.9298\n",
      "Epoch 1/3 | Batch 450/950 | Loss: 5.9012\n",
      "Epoch 1/3 | Batch 500/950 | Loss: 5.8715\n",
      "Epoch 1/3 | Batch 550/950 | Loss: 5.8462\n",
      "Epoch 1/3 | Batch 600/950 | Loss: 5.8279\n",
      "Epoch 1/3 | Batch 650/950 | Loss: 5.8033\n",
      "Epoch 1/3 | Batch 700/950 | Loss: 5.7844\n",
      "Epoch 1/3 | Batch 750/950 | Loss: 5.7662\n",
      "Epoch 1/3 | Batch 800/950 | Loss: 5.7521\n",
      "Epoch 1/3 | Batch 850/950 | Loss: 5.7368\n",
      "Epoch 1/3 | Batch 900/950 | Loss: 5.7260\n",
      "Epoch 1/3 | Batch 950/950 | Loss: 5.7132\n",
      "‚úÖ Epoch 1 done | Avg Loss: 5.7132\n",
      "\n",
      "Epoch 2/3 | Batch 50/950 | Loss: 5.4817\n",
      "Epoch 2/3 | Batch 100/950 | Loss: 5.4540\n"
     ]
    }
   ],
   "source": [
    "# === TRAINING CONFIG ===\n",
    "epochs = 3\n",
    "device = 'cuda'\n",
    "\n",
    "print(f\"Starting training...\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# === TRAINING ===\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move to GPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print co 50 batchy\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Batch {batch_idx+1}/{len(train_loader)} | Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Epoch summary\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} done | Avg Loss: {avg_loss:.4f}\\n\")\n",
    "\n",
    "print(\"üéâ Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17141f1-bc99-4c1c-abc9-625cffdb8a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST PREDICTION ===\n",
    "model.eval()\n",
    "\n",
    "# We≈∫ przyk≈Çad z val\n",
    "sample = val_dataset[0]\n",
    "input_ids = sample['input_ids'].unsqueeze(0).to(device)  # [1, 13]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits  # [1, 13, 853]\n",
    "    \n",
    "    # Prawdopodobie≈Ñstwa dla ostatniej pozycji\n",
    "    last_logits = logits[0, -1, :]  # [853]\n",
    "    probs = torch.softmax(last_logits, dim=0)\n",
    "    \n",
    "    # Top-5 najbardziej prawdopodobnych token√≥w\n",
    "    top5_probs, top5_ids = torch.topk(probs, 5)\n",
    "    \n",
    "    print(\"=== PREDICTION ===\")\n",
    "    print(f\"Input: {[id_to_token[id] for id in input_ids[0].tolist() if id != 0][:5]}...\")\n",
    "    print(f\"\\nTop 5 next tokens:\")\n",
    "    for prob, id in zip(top5_probs, top5_ids):\n",
    "        token = id_to_token[id.item()]\n",
    "        print(f\"  {token:30s} {prob.item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c8a3a-5f82-453d-9dbe-88c63180fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === ANALIZA D≈ÅUGO≈öCI SEKWENCJI W VAL SET ===\n",
    "\n",
    "print(\"=== VALIDATION SET - SEQUENCE LENGTH ANALYSIS ===\\n\")\n",
    "\n",
    "# Podstawowe statystyki\n",
    "print(\"Basic stats:\")\n",
    "print(val_df['sequence_length'].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DISTRIBUTION BY LENGTH:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Rozk≈Çad szczeg√≥≈Çowy\n",
    "length_dist = val_df['sequence_length'].value_counts().sort_index()\n",
    "\n",
    "for length, count in length_dist.items():\n",
    "    pct = count / len(val_df) * 100\n",
    "    bar = \"‚ñà\" * int(pct / 2)  # wizualizacja\n",
    "    print(f\"Length {length:2d}: {count:4d} ({pct:5.2f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e526383-32c7-456d-839e-1536ca2b12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F_torch  # ‚Üê DODAJ TO!\n",
    "from pyspark.sql import functions as F  # ‚Üê to zostaje dla PySpark\n",
    "\n",
    "# === FUNKCJA 1: Find Closest Token ===\n",
    "def find_closest_token(target_token, vocab, token_type='START_LOC'):\n",
    "    \"\"\"Znajd≈∫ najbli≈ºszy token w vocab.\"\"\"\n",
    "    parts = target_token.split('_')\n",
    "    target_x = int(parts[2])\n",
    "    target_y = int(parts[3])\n",
    "    \n",
    "    available = [t for t in vocab.keys() if t.startswith(token_type)]\n",
    "    \n",
    "    if not available:\n",
    "        return None, float('inf')\n",
    "    \n",
    "    min_dist = float('inf')\n",
    "    closest = None\n",
    "    \n",
    "    for token in available:\n",
    "        parts = token.split('_')\n",
    "        x = int(parts[2])\n",
    "        y = int(parts[3])\n",
    "        dist = ((x - target_x)**2 + (y - target_y)**2)**0.5\n",
    "        \n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest = token\n",
    "    \n",
    "    return closest, min_dist\n",
    "\n",
    "\n",
    "# === FUNKCJA 2: Monte Carlo FAST (batched) ===\n",
    "def calculate_xT_montecarlo_safe(model, start_tokens, vocab, id_to_token, \n",
    "                                   n_rollouts=200, n_steps=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Monte Carlo xT z CONSTRAINED DECODING.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_valid_token_ids(previous_token, vocab):\n",
    "        \"\"\"Zwr√≥ƒá IDs dozwolonych token√≥w.\"\"\"\n",
    "        if previous_token == 'SHOT':\n",
    "            valid = ['GOAL', 'NO_GOAL']\n",
    "        elif previous_token in ['GOAL', 'NO_GOAL']:\n",
    "            valid = []  # koniec\n",
    "        else:\n",
    "            valid = [t for t in vocab.keys() \n",
    "                     if t.startswith('Pass') or t.startswith('Carry') or t == 'SHOT']\n",
    "        \n",
    "        return [vocab[t] for t in valid if t in vocab]\n",
    "    \n",
    "    model.eval()\n",
    "    goal_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for rollout in range(n_rollouts):\n",
    "            current_tokens = start_tokens.copy()\n",
    "            \n",
    "            for step in range(n_steps):\n",
    "                # Encode\n",
    "                input_ids = torch.tensor([vocab[t] for t in current_tokens]).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Forward\n",
    "                outputs = model(input_ids)\n",
    "                logits = outputs.logits[0, -1, :]  # ostatni token\n",
    "                \n",
    "                # === CONSTRAINED DECODING ===\n",
    "                previous_token = current_tokens[-1]\n",
    "                valid_ids = get_valid_token_ids(previous_token, vocab)\n",
    "                \n",
    "                if not valid_ids:  # koniec (po GOAL/NO_GOAL)\n",
    "                    break\n",
    "                \n",
    "                # Maskuj\n",
    "                mask = torch.zeros_like(logits)\n",
    "                mask[valid_ids] = 1.0\n",
    "                logits_masked = logits + (mask - 1.0) * 1e10  # -inf dla niedozwolonych\n",
    "                \n",
    "                # Sample\n",
    "                probs = F_torch.softmax(logits_masked, dim=-1)  # ‚úÖ ZMIENIONE!\n",
    "                next_id = torch.multinomial(probs, 1).item()\n",
    "                next_token = id_to_token[next_id]\n",
    "                \n",
    "                current_tokens.append(next_token)\n",
    "                \n",
    "                # Check outcome\n",
    "                if next_token == 'GOAL':\n",
    "                    goal_count += 1\n",
    "                    break\n",
    "                elif next_token == 'NO_GOAL':\n",
    "                    break\n",
    "    \n",
    "    return goal_count / n_rollouts\n",
    "\n",
    "\n",
    "# === FUNKCJA 3: Evaluate na val set ===\n",
    "def evaluate_xT_on_val_set(model, val_df, vocab, id_to_token, \n",
    "                            n_rollouts=200, n_steps=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Oblicz xT dla ca≈Çego val set.\n",
    "    U≈ºywa FAST (batched) implementacji calculate_xT_montecarlo_safe.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    print(f\"Evaluating on {len(val_df)} validation sequences...\")\n",
    "    print(f\"Rollouts per sequence: {n_rollouts}\")\n",
    "    \n",
    "    for idx in range(len(val_df)):\n",
    "        sequence = val_df['full_sequence'].iloc[idx]\n",
    "        true_label = val_df['goal'].iloc[idx]\n",
    "        \n",
    "        if len(sequence) == 3:\n",
    "            start_tokens = list(sequence[:2])  # 2 tokeny dla length=3\n",
    "        else:\n",
    "            start_tokens = list(sequence[:3])  # 3 tokeny dla length>=4\n",
    "        \n",
    "        try:\n",
    "            xT = calculate_xT_montecarlo_safe(\n",
    "                model, start_tokens, vocab, id_to_token,\n",
    "                n_rollouts=n_rollouts, n_steps=n_steps, device=device\n",
    "            )\n",
    "            predictions.append(xT)\n",
    "            true_labels.append(true_label)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error on sequence {idx}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if (idx + 1) % 100 == 0:\n",
    "            avg_xt = np.mean(predictions)\n",
    "            print(f\"Processed {idx+1}/{len(val_df)} | Avg xT: {avg_xt*100:.2f}%\")\n",
    "    \n",
    "    return np.array(predictions), np.array(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada778b2-f704-4c33-8d04-fd896c2bb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = evaluate_xT_on_val_set(\n",
    "    model, val_df, vocab, id_to_token,\n",
    "    n_rollouts=100,\n",
    "    n_steps=5\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(labels, predictions)\n",
    "print(f\"\\nüéØ ROC-AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1add8-3e97-440a-bde7-2427ec4061d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# === SETUP MLFLOW ===\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")  # lokalny folder\n",
    "mlflow.set_experiment(\"xT_LLM_experiments\")\n",
    "import os\n",
    "\n",
    "# Utw√≥rz folder artifacts\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "print(\"‚úÖ Artifacts folder created\")\n",
    "print(\"‚úÖ MLflow configured\")\n",
    "\n",
    "# === START RUN ===\n",
    "with mlflow.start_run(run_name=\"xT_verb\"):\n",
    "    \n",
    "    print(\"üìù Logging parameters...\")\n",
    "    \n",
    "    # === LOG PARAMETERS ===\n",
    "    # Model config\n",
    "    mlflow.log_param(\"model_architecture\", \"GPT-2 Small\")\n",
    "    mlflow.log_param(\"vocab_size\", len(vocab))\n",
    "    mlflow.log_param(\"max_seq_length\", 14)\n",
    "    mlflow.log_param(\"model_parameters\", sum(p.numel() for p in model.parameters()))\n",
    "    \n",
    "    # Training config\n",
    "    mlflow.log_param(\"epochs\", 3)\n",
    "    mlflow.log_param(\"learning_rate\", 5e-4)\n",
    "    mlflow.log_param(\"batch_size\", 16)\n",
    "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
    "    mlflow.log_param(\"loss_function\", \"CrossEntropyLoss\")\n",
    "    \n",
    "    # Data config\n",
    "    mlflow.log_param(\"train_sequences\", len(train_df))\n",
    "    mlflow.log_param(\"val_sequences\", len(val_df))\n",
    "    mlflow.log_param(\"train_goal_rate\", f\"{train_df['goal'].mean()*100:.2f}%\")\n",
    "    mlflow.log_param(\"val_goal_rate\", f\"{val_df['goal'].mean()*100:.2f}%\")\n",
    "    mlflow.log_param(\"leagues\", \"SerieA_2015_2016\")\n",
    "    \n",
    "    # Evaluation config\n",
    "    mlflow.log_param(\"uwagi_podejscie\", \"obr√≥cone warto≈õci x,y; brak kƒÖta strza≈Çu\")\n",
    "\n",
    "    \n",
    "    print(\"üìä Logging metrics...\")\n",
    "    \n",
    "    # === CALCULATE & LOG METRICS ===\n",
    "    roc_auc = roc_auc_score(labels, predictions)\n",
    "    avg_precision = average_precision_score(labels, predictions)\n",
    "    brier = brier_score_loss(labels, predictions)\n",
    "    \n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    mlflow.log_metric(\"avg_precision\", avg_precision)\n",
    "    mlflow.log_metric(\"brier_score\", brier)\n",
    "    mlflow.log_metric(\"mean_predicted_xT\", predictions.mean())\n",
    "    mlflow.log_metric(\"std_predicted_xT\", predictions.std())\n",
    "    mlflow.log_metric(\"min_predicted_xT\", predictions.min())\n",
    "    mlflow.log_metric(\"max_predicted_xT\", predictions.max())\n",
    "    mlflow.log_metric(\"median_predicted_xT\", float(np.median(predictions)))\n",
    "    \n",
    "    # Class-specific metrics\n",
    "    mlflow.log_metric(\"mean_xT_for_goals\", predictions[labels==1].mean())\n",
    "    mlflow.log_metric(\"mean_xT_for_no_goals\", predictions[labels==0].mean())\n",
    "    \n",
    "    print(\"üìà Creating plots...\")\n",
    "    \n",
    "    # === CREATE & LOG PLOTS ===\n",
    "    \n",
    "    # 1. ROC Curve\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f'Model (AUC={roc_auc:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/roc_curve.png', dpi=150)\n",
    "    mlflow.log_artifact('artifacts/roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Precision-Recall Curve\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions)\n",
    "    baseline = labels.mean()\n",
    "    ax.plot(recall, precision, linewidth=2, label=f'Model (AP={avg_precision:.3f})')\n",
    "    ax.axhline(y=baseline, color='k', linestyle='--', linewidth=1, \n",
    "               label=f'Baseline ({baseline*100:.1f}%)')\n",
    "    ax.set_xlabel('Recall', fontsize=12)\n",
    "    ax.set_ylabel('Precision', fontsize=12)\n",
    "    ax.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/pr_curve.png', dpi=150)\n",
    "    mlflow.log_artifact('artifacts/pr_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Prediction Distribution\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.hist(predictions[labels==0], bins=40, alpha=0.6, label='No Goal', \n",
    "            color='blue', density=True, edgecolor='black', linewidth=0.5)\n",
    "    ax.hist(predictions[labels==1], bins=40, alpha=0.6, label='Goal', \n",
    "            color='red', density=True, edgecolor='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Predicted xT', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Prediction Distribution by Outcome', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/prediction_dist.png', dpi=150)\n",
    "    mlflow.log_artifact('artifacts/prediction_dist.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Calibration Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    n_bins = 10\n",
    "    bin_edges = np.linspace(0, predictions.max(), n_bins + 1)\n",
    "    bin_centers = []\n",
    "    actual_rates = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = (predictions >= bin_edges[i]) & (predictions < bin_edges[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            bin_centers.append((bin_edges[i] + bin_edges[i+1]) / 2)\n",
    "            actual_rates.append(labels[mask].mean())\n",
    "    \n",
    "    ax.plot(bin_centers, actual_rates, 'o-', linewidth=2, markersize=8, label='Model')\n",
    "    ax.plot([0, max(bin_centers)], [0, max(bin_centers)], 'k--', linewidth=1, label='Perfect calibration')\n",
    "    ax.set_xlabel('Predicted xT (binned)', fontsize=12)\n",
    "    ax.set_ylabel('Actual Goal Rate', fontsize=12)\n",
    "    ax.set_title('Calibration Plot', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/calibration.png', dpi=150)\n",
    "    mlflow.log_artifact('artifacts/calibration.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"üíæ Saving artifacts...\")\n",
    "    \n",
    "    # === SAVE DATA ARTIFACTS ===\n",
    "    \n",
    "    # Predictions & labels\n",
    "    results_df = pd.DataFrame({\n",
    "        'prediction': predictions,\n",
    "        'label': labels,\n",
    "        'sequence_index': range(len(predictions))\n",
    "    })\n",
    "    results_df.to_csv('artifacts/predictions.csv', index=False)\n",
    "    mlflow.log_artifact('artifacts/predictions.csv')\n",
    "    \n",
    "    # Vocabulary\n",
    "    with open('artifacts/vocab.json', 'w') as f:\n",
    "        json.dump(vocab, f, indent=2)\n",
    "    mlflow.log_artifact('artifacts/vocab.json')\n",
    "    \n",
    "    # Summary stats\n",
    "    summary = {\n",
    "        'model': 'GPT-2 Small baseline',\n",
    "        'metrics': {\n",
    "            'roc_auc': float(roc_auc),\n",
    "            'avg_precision': float(avg_precision),\n",
    "            'brier_score': float(brier)\n",
    "        },\n",
    "        'predictions': {\n",
    "            'mean': float(predictions.mean()),\n",
    "            'std': float(predictions.std()),\n",
    "            'min': float(predictions.min()),\n",
    "            'max': float(predictions.max())\n",
    "        },\n",
    "        'separation': {\n",
    "            'mean_xT_goals': float(predictions[labels==1].mean()),\n",
    "            'mean_xT_no_goals': float(predictions[labels==0].mean()),\n",
    "            'difference': float(predictions[labels==1].mean() - predictions[labels==0].mean())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('artifacts/summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    mlflow.log_artifact('artifacts/summary.json')\n",
    "    \n",
    "    print(\"‚úÖ Run logged successfully!\")\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"RUN SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Avg Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Brier Score: {brier:.4f}\")\n",
    "    print(f\"Mean xT (goals): {predictions[labels==1].mean()*100:.2f}%\")\n",
    "    print(f\"Mean xT (no goals): {predictions[labels==0].mean()*100:.2f}%\")\n",
    "    print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785b269-ba6b-4604-9e24-e97d4b9cb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# === FUNKCJA DO INSPEKCJI OBSERWACJI ===\n",
    "def inspect_validation_sample(val_df, idx, predictions=None, labels=None):\n",
    "    \"\"\"\n",
    "    Szczeg√≥≈Çowa inspekcja pojedynczej obserwacji z validation set.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"VALIDATION SAMPLE #{idx}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Podstawowe info\n",
    "    sequence = val_df.iloc[idx]['full_sequence']\n",
    "    true_label = val_df.iloc[idx]['goal']\n",
    "    seq_length = len(sequence)\n",
    "    \n",
    "    print(f\"\\nüìä Basic Info:\")\n",
    "    print(f\"  Sequence length: {seq_length}\")\n",
    "    print(f\"  True outcome: {'GOAL ‚öΩ' if true_label == 1 else 'NO_GOAL ‚ùå'}\")\n",
    "    \n",
    "    # Pe≈Çna sekwencja\n",
    "    print(f\"\\nüìù Full Sequence:\")\n",
    "    for i, token in enumerate(sequence):\n",
    "        marker = \"  \"\n",
    "        if i < 3:  # kontekst u≈ºywany w evaluation\n",
    "            marker = \"‚Üí \"  # kontekst\n",
    "        elif i == len(sequence) - 1:  # outcome\n",
    "            marker = \"üéØ\"\n",
    "        print(f\"  {marker} {i}: {token}\")\n",
    "    \n",
    "    # Kontekst u≈ºywany w Monte Carlo\n",
    "    if seq_length == 3:\n",
    "        context_tokens = list(sequence[:2])\n",
    "        print(f\"\\nüîç Context used (length=3, only 2 tokens):\")\n",
    "    else:\n",
    "        context_tokens = list(sequence[:3])\n",
    "        print(f\"\\nüîç Context used (first 3 tokens):\")\n",
    "    \n",
    "    for i, token in enumerate(context_tokens):\n",
    "        print(f\"  ‚Üí {token}\")\n",
    "    \n",
    "    # Predykcja (je≈õli podana)\n",
    "    if predictions is not None and labels is not None:\n",
    "        pred_xT = predictions[idx]\n",
    "        print(f\"\\nü§ñ Model Prediction:\")\n",
    "        print(f\"  xT (5 steps): {pred_xT*100:.2f}%\")\n",
    "        print(f\"  True label: {true_label} ({'GOAL' if true_label == 1 else 'NO_GOAL'})\")\n",
    "        \n",
    "        # Klasyfikacja\n",
    "        if true_label == 1:  # faktyczny GOAL\n",
    "            if pred_xT > 0.10:  # high xT\n",
    "                result = \"‚úÖ TRUE POSITIVE (high xT, actual GOAL)\"\n",
    "            else:\n",
    "                result = \"‚ùå FALSE NEGATIVE (low xT, but actual GOAL)\"\n",
    "        else:  # faktyczny NO_GOAL\n",
    "            if pred_xT > 0.10:\n",
    "                result = \"‚ùå FALSE POSITIVE (high xT, but NO_GOAL)\"\n",
    "            else:\n",
    "                result = \"‚úÖ TRUE NEGATIVE (low xT, no goal)\"\n",
    "        \n",
    "        print(f\"  Classification: {result}\")\n",
    "        \n",
    "        # Analiza xT\n",
    "        if pred_xT > 0.15:\n",
    "            threat = \"üî¥ HIGH THREAT\"\n",
    "        elif pred_xT > 0.08:\n",
    "            threat = \"üü° MEDIUM THREAT\"\n",
    "        else:\n",
    "            threat = \"üü¢ LOW THREAT\"\n",
    "        print(f\"  Threat level: {threat}\")\n",
    "    \n",
    "    # Dekoduj lokalizacje (je≈õli mo≈ºliwe)\n",
    "    print(f\"\\nüó∫Ô∏è  Location Analysis:\")\n",
    "    for i, token in enumerate(context_tokens):\n",
    "        if '_LOC_' in token:\n",
    "            parts = token.split('_')\n",
    "            event_type = parts[0]\n",
    "            x = int(parts[2])\n",
    "            y = int(parts[3])\n",
    "            \n",
    "            # Okre≈õl strefƒô\n",
    "            if x < 60:\n",
    "                zone = \"Defensive third\"\n",
    "            elif x < 102:\n",
    "                zone = \"Middle third\"\n",
    "            else:\n",
    "                zone = \"‚ö†Ô∏è ATTACKING THIRD (penalty area!)\"\n",
    "            \n",
    "            print(f\"  {i+1}. {event_type} at ({x}, {y}) - {zone}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "\n",
    "# === INSPEKCJA WYBRANYCH OBSERWACJI ===\n",
    "indices_to_inspect = [197, 3, 246]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DETAILED VALIDATION SAMPLE INSPECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx in indices_to_inspect:\n",
    "    # Sprawd≈∫ czy masz predictions/labels (z poprzedniej evaluation)\n",
    "    if 'predictions' in globals() and 'labels' in globals():\n",
    "        inspect_validation_sample(val_df, idx, predictions, labels)\n",
    "    else:\n",
    "        inspect_validation_sample(val_df, idx)\n",
    "\n",
    "# === POR√ìWNANIE 3 OBSERWACJI ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = []\n",
    "for idx in indices_to_inspect:\n",
    "    seq = val_df.iloc[idx]['full_sequence']\n",
    "    true_label = val_df.iloc[idx]['goal']\n",
    "    \n",
    "    if 'predictions' in globals():\n",
    "        pred = predictions[idx]\n",
    "    else:\n",
    "        pred = None\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'idx': idx,\n",
    "        'length': len(seq),\n",
    "        'first_token': seq[0],\n",
    "        'last_token': seq[-1],\n",
    "        'true_label': true_label,\n",
    "        'pred_xT': pred\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "comp_df = pd.DataFrame(comparison_data)\n",
    "print(comp_df.to_string(index=False))\n",
    "\n",
    "# === WIZUALIZACJA POZYCJI (opcjonalne) ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"POSITION VISUALIZATION (ASCII)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def plot_positions_ascii(sequences, idx):\n",
    "    \"\"\"Prosta wizualizacja pozycji na boisku (ASCII).\"\"\"\n",
    "    print(f\"\\nSample #{idx}:\")\n",
    "    \n",
    "    # Boisko 120x80\n",
    "    field = [[' ' for _ in range(24)] for _ in range(16)]  # scaled down 5x\n",
    "    \n",
    "    seq = sequences.iloc[idx]['full_sequence']\n",
    "    context = seq[:3] if len(seq) > 3 else seq[:2]\n",
    "    \n",
    "    positions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83555a88-f519-4a99-bb8f-b424924f9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern w training set:\n",
    "target_pattern = ['START_LOC_110_40', 'SHOT_ANG_35']\n",
    "\n",
    "train_count = 0\n",
    "train_goals = 0\n",
    "\n",
    "for seq in train_df['full_sequence']:\n",
    "    if len(seq) >= 2:\n",
    "        if list(seq[:2]) == target_pattern:\n",
    "            train_count += 1\n",
    "            if len(seq) == 3 and seq[2] == 'GOAL':\n",
    "                train_goals += 1\n",
    "\n",
    "print(f\"=== PATTERN ANALYSIS IN TRAINING ===\")\n",
    "print(f\"Pattern occurrences: {train_count}\")\n",
    "print(f\"Pattern ‚Üí GOAL: {train_goals}\")\n",
    "if train_count > 0:\n",
    "    print(f\"Conversion rate: {train_goals / train_count * 100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
